log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel("INFO")
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.3
      /_/

Using Scala version 2.10.5 (OpenJDK 64-Bit Server VM, Java 1.8.0_402)
Type in expressions to have them evaluated.
Type :help for more information.
24/02/04 16:08:18 WARN Utils: Your hostname, DESKTOP-ENDFM1D resolves to a loopback address: 127.0.1.1; using 172.26.95.204 instead (on interface eth0)
24/02/04 16:08:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Spark context available as sc.
24/02/04 16:08:21 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
24/02/04 16:08:21 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
24/02/04 16:08:24 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
24/02/04 16:08:25 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/02/04 16:08:27 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
24/02/04 16:08:27 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
SQL context available as sqlContext.

scala> import org.apache.spark.sql._
import org.apache.spark.sql._

scala> import org.apache.spark.sql.types._
import org.apache.spark.sql.types._

scala> 

scala> val sparkVersion = sc.version
sparkVersion: String = 1.6.3

scala> 

scala> val testId = sc.getConf.get("spark.args.testId")
testId: String = test__1.6.3__2.6.5__8__3

scala> 

scala> sqlContext.sql("SET spark.sql.parquet.compression.codec=uncompressed")
res0: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> // spark.sql("SET spark.sql.parquet.int96AsTimestamp=true")

scala> 

scala> val data = Seq(
     |     Row("1", "0000-01-01"),
     |     Row("2", "0050-01-01"),
     |     Row("3", "0999-12-31"),
     |     Row("4", "1000-01-01"),
     |     Row("5", "1592-10-14"),
     |     Row("6", "1592-10-15"),
     |     Row("7", "1899-12-31"),
     |     Row("8", "1900-01-01"),
     |     Row("9", "9999-12-31")
     | )
data: Seq[org.apache.spark.sql.Row] = List([1,0000-01-01], [2,0050-01-01], [3,0999-12-31], [4,1000-01-01], [5,1592-10-14], [6,1592-10-15], [7,1899-12-31], [8,1900-01-01], [9,9999-12-31])

scala> 

scala> val rdd = sc.parallelize(data)
rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[1] at parallelize at <console>:35

scala> 

scala> val schema = new StructType(Array(StructField("id", StringType), StructField("date", StringType)))
schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(date,StringType,true))

scala> 

scala> var dfw = sqlContext.createDataFrame(rdd, schema)
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string]

scala> dfw.registerTempTable("dfw")

scala> dfw = sqlContext.sql("SELECT id, date, CAST(date AS DATE) AS dt, CAST(CONCAT(date, ' 00:00:00') AS TIMESTAMP) AS ts, NAMED_STRUCT('dt', CAST(date AS DATE), 'ts', CAST(CONCAT(date, ' 00:00:00') AS TIMESTAMP)) AS nested FROM dfw")
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>]

scala> 

scala> dfw = dfw.withColumn("ctxSparkVersion", lit(sparkVersion))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>, ctxSparkVersion: string]

scala> dfw = dfw.withColumn("argSparkVersion", lit(sc.getConf.get("spark.args.sparkVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>, ctxSparkVersion: string, argSparkVersion: string]

scala> dfw = dfw.withColumn("argHadoopVersion", lit(sc.getConf.get("spark.args.hadoopVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>, ctxSparkVersion: string, argSparkVersion: string, argHadoopVersion: string]

scala> dfw = dfw.withColumn("argTwitterParquetCommonVersion", lit(sc.getConf.get("spark.args.twitterParquetCommonVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>, ctxSparkVersion: string, argSparkVersion: string, argHadoopVersion: string, argTwitterParquetCommonVersion: string]

scala> dfw = dfw.withColumn("argTwitterParquetFormatVersion", lit(sc.getConf.get("spark.args.twitterParquetFormatVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>, ctxSparkVersion: string, argSparkVersion: string, argHadoopVersion: string, argTwitterParquetCommonVersion: string, argTwitterParquetFormatVersion: string]

scala> dfw = dfw.withColumn("argTwitterParquetThriftVersion", lit(sc.getConf.get("spark.args.twitterParquetThriftVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>, ctxSparkVersion: string, argSparkVersion: string, argHadoopVersion: string, argTwitterParquetCommonVersion: string, argTwitterParquetFormatVersion: string, argTwitterParquetThriftVersion: string]

scala> dfw = dfw.withColumn("argApacheParquetCommonVersion", lit(sc.getConf.get("spark.args.apacheParquetCommonVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>, ctxSparkVersion: string, argSparkVersion: string, argHadoopVersion: string, argTwitterParquetCommonVersion: string, argTwitterParquetFormatVersion: string, argTwitterParquetThriftVersion: string, argApacheParquetCommonVersion: string]

scala> dfw = dfw.withColumn("argApacheParquetFormatVersion", lit(sc.getConf.get("spark.args.apacheParquetFormatVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>, ctxSparkVersion: string, argSparkVersion: string, argHadoopVersion: string, argTwitterParquetCommonVersion: string, argTwitterParquetFormatVersion: string, argTwitterParquetThriftVersion: string, argApacheParquetCommonVersion: string, argApacheParquetFormatVersion: string]

scala> dfw = dfw.withColumn("argApacheParquetThriftVersion", lit(sc.getConf.get("spark.args.apacheParquetThriftVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>, ctxSparkVersion: string, argSparkVersion: string, argHadoopVersion: string, argTwitterParquetCommonVersion: string, argTwitterParquetFormatVersion: string, argTwitterParquetThriftVersion: string, argApacheParquetCommonVersion: string, argApacheParquetFormatVersion: string, argApacheParquetThriftVersion: string]

scala> 

scala> dfw.printSchema()
root
 |-- id: string (nullable = true)
 |-- date: string (nullable = true)
 |-- dt: date (nullable = true)
 |-- ts: timestamp (nullable = true)
 |-- nested: struct (nullable = false)
 |    |-- dt: date (nullable = true)
 |    |-- ts: timestamp (nullable = true)
 |-- ctxSparkVersion: string (nullable = false)
 |-- argSparkVersion: string (nullable = false)
 |-- argHadoopVersion: string (nullable = false)
 |-- argTwitterParquetCommonVersion: string (nullable = false)
 |-- argTwitterParquetFormatVersion: string (nullable = false)
 |-- argTwitterParquetThriftVersion: string (nullable = false)
 |-- argApacheParquetCommonVersion: string (nullable = false)
 |-- argApacheParquetFormatVersion: string (nullable = false)
 |-- argApacheParquetThriftVersion: string (nullable = false)


scala> dfw.show()
+---+----------+----------+--------------------+--------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+
| id|      date|        dt|                  ts|              nested|ctxSparkVersion|argSparkVersion|argHadoopVersion|argTwitterParquetCommonVersion|argTwitterParquetFormatVersion|argTwitterParquetThriftVersion|argApacheParquetCommonVersion|argApacheParquetFormatVersion|argApacheParquetThriftVersion|
+---+----------+----------+--------------------+--------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+
|  1|0000-01-01|0001-01-01|0001-01-01 00:00:...|[0001-01-01,0001-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  2|0050-01-01|0050-01-01|0050-01-01 00:00:...|[0050-01-01,0050-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  3|0999-12-31|0999-12-31|0999-12-31 00:00:...|[0999-12-31,0999-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  4|1000-01-01|1000-01-01|1000-01-01 00:00:...|[1000-01-01,1000-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  5|1592-10-14|1592-10-14|1592-10-14 00:00:...|[1592-10-14,1592-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  6|1592-10-15|1592-10-15|1592-10-15 00:00:...|[1592-10-15,1592-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  7|1899-12-31|1899-12-31|1899-12-31 00:00:...|[1899-12-31,1899-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  8|1900-01-01|1900-01-01|1900-01-01 00:00:...|[1900-01-01,1900-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  9|9999-12-31|9999-12-31|9999-12-31 00:00:...|[9999-12-31,9999-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
+---+----------+----------+--------------------+--------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+


scala> 

scala> val loc = s"/home/gmanche/data/testId=${testId}"
loc: String = /home/gmanche/data/testId=test__1.6.3__2.6.5__8__3

scala> 

scala> dfw.repartition(1).write.format("parquet").mode("overwrite").save(loc)
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

scala> 

scala> val dfr = sqlContext.read.format("parquet").load(loc)
dfr: org.apache.spark.sql.DataFrame = [id: string, date: string, dt: date, ts: timestamp, nested: struct<dt:date,ts:timestamp>, ctxSparkVersion: string, argSparkVersion: string, argHadoopVersion: string, argTwitterParquetCommonVersion: string, argTwitterParquetFormatVersion: string, argTwitterParquetThriftVersion: string, argApacheParquetCommonVersion: string, argApacheParquetFormatVersion: string, argApacheParquetThriftVersion: string]

scala> 

scala> dfr.printSchema()
root
 |-- id: string (nullable = true)
 |-- date: string (nullable = true)
 |-- dt: date (nullable = true)
 |-- ts: timestamp (nullable = true)
 |-- nested: struct (nullable = true)
 |    |-- dt: date (nullable = true)
 |    |-- ts: timestamp (nullable = true)
 |-- ctxSparkVersion: string (nullable = true)
 |-- argSparkVersion: string (nullable = true)
 |-- argHadoopVersion: string (nullable = true)
 |-- argTwitterParquetCommonVersion: string (nullable = true)
 |-- argTwitterParquetFormatVersion: string (nullable = true)
 |-- argTwitterParquetThriftVersion: string (nullable = true)
 |-- argApacheParquetCommonVersion: string (nullable = true)
 |-- argApacheParquetFormatVersion: string (nullable = true)
 |-- argApacheParquetThriftVersion: string (nullable = true)


scala> dfr.show()
24/02/04 16:08:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
+---+----------+----------+--------------------+--------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+
| id|      date|        dt|                  ts|              nested|ctxSparkVersion|argSparkVersion|argHadoopVersion|argTwitterParquetCommonVersion|argTwitterParquetFormatVersion|argTwitterParquetThriftVersion|argApacheParquetCommonVersion|argApacheParquetFormatVersion|argApacheParquetThriftVersion|
+---+----------+----------+--------------------+--------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+
|  1|0000-01-01|0001-01-01|0001-01-01 00:00:...|[0001-01-01,0001-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  2|0050-01-01|0050-01-01|0050-01-01 00:00:...|[0050-01-01,0050-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  3|0999-12-31|0999-12-31|0999-12-31 00:00:...|[0999-12-31,0999-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  4|1000-01-01|1000-01-01|1000-01-01 00:00:...|[1000-01-01,1000-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  5|1592-10-14|1592-10-14|1592-10-14 00:00:...|[1592-10-14,1592-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  6|1592-10-15|1592-10-15|1592-10-15 00:00:...|[1592-10-15,1592-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  7|1899-12-31|1899-12-31|1899-12-31 00:00:...|[1899-12-31,1899-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  8|1900-01-01|1900-01-01|1900-01-01 00:00:...|[1900-01-01,1900-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
|  9|9999-12-31|9999-12-31|9999-12-31 00:00:...|[9999-12-31,9999-...|          1.6.3|          1.6.3|           2.6.5|                         1.6.0|                      2.2.0rc1|                         0.7.0|                        1.7.0|             2.3.0-incubating|                        0.7.0|
+---+----------+----------+--------------------+--------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+


scala> Stopping spark context.
