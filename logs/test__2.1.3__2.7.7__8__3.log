Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/02/04 16:09:09 WARN Utils: Your hostname, DESKTOP-ENDFM1D resolves to a loopback address: 127.0.1.1; using 172.26.95.204 instead (on interface eth0)
24/02/04 16:09:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Spark context Web UI available at http://172.26.95.204:4040
Spark context available as 'sc' (master = local[*], app id = local-1707059350697).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.3
      /_/
         
Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_402)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.sql._
import org.apache.spark.sql._

scala> import org.apache.spark.sql.types._
import org.apache.spark.sql.types._

scala> 

scala> val sparkVersion = sc.version
sparkVersion: String = 2.1.3

scala> 

scala> val testId = spark.conf.get("spark.args.testId")
24/02/04 16:09:16 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
testId: String = test__2.1.3__2.7.7__8__3

scala> 

scala> spark.sql("SET spark.sql.parquet.compression.codec=uncompressed")
res0: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> // spark.sql("SET spark.sql.parquet.int96AsTimestamp=true")

scala> 

scala> val data = Seq(
     |     Row("1", "0000-01-01"),
     |     Row("2", "0050-01-01"),
     |     Row("3", "0999-12-31"),
     |     Row("4", "1000-01-01"),
     |     Row("5", "1592-10-14"),
     |     Row("6", "1592-10-15"),
     |     Row("7", "1899-12-31"),
     |     Row("8", "1900-01-01"),
     |     Row("9", "9999-12-31")
     | )
data: Seq[org.apache.spark.sql.Row] = List([1,0000-01-01], [2,0050-01-01], [3,0999-12-31], [4,1000-01-01], [5,1592-10-14], [6,1592-10-15], [7,1899-12-31], [8,1900-01-01], [9,9999-12-31])

scala> 

scala> val rdd = sc.parallelize(data)
rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[1] at parallelize at <console>:32

scala> 

scala> val schema = new StructType().add("id", StringType).add("date", StringType)
schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(date,StringType,true))

scala> 

scala> var dfw = spark.createDataFrame(rdd, schema)
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string]

scala> dfw.createOrReplaceTempView("dfw")

scala> dfw = spark.sql("SELECT id, date, CAST(date AS DATE) AS dt, CAST(CONCAT(date, ' 00:00:00') AS TIMESTAMP) AS ts, NAMED_STRUCT('dt', CAST(date AS DATE), 'ts', CAST(CONCAT(date, ' 00:00:00') AS TIMESTAMP)) AS nested FROM dfw")
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string ... 3 more fields]

scala> 

scala> dfw = dfw.withColumn("ctxSparkVersion", lit(sparkVersion))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string ... 4 more fields]

scala> dfw = dfw.withColumn("argSparkVersion", lit(spark.conf.get("spark.args.sparkVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string ... 5 more fields]

scala> dfw = dfw.withColumn("argHadoopVersion", lit(spark.conf.get("spark.args.hadoopVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string ... 6 more fields]

scala> dfw = dfw.withColumn("argTwitterParquetCommonVersion", lit(spark.conf.get("spark.args.twitterParquetCommonVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string ... 7 more fields]

scala> dfw = dfw.withColumn("argTwitterParquetFormatVersion", lit(spark.conf.get("spark.args.twitterParquetFormatVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string ... 8 more fields]

scala> dfw = dfw.withColumn("argTwitterParquetThriftVersion", lit(spark.conf.get("spark.args.twitterParquetThriftVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string ... 9 more fields]

scala> dfw = dfw.withColumn("argApacheParquetCommonVersion", lit(spark.conf.get("spark.args.apacheParquetCommonVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string ... 10 more fields]

scala> dfw = dfw.withColumn("argApacheParquetFormatVersion", lit(spark.conf.get("spark.args.apacheParquetFormatVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string ... 11 more fields]

scala> dfw = dfw.withColumn("argApacheParquetThriftVersion", lit(spark.conf.get("spark.args.apacheParquetThriftVersion")))
dfw: org.apache.spark.sql.DataFrame = [id: string, date: string ... 12 more fields]

scala> 

scala> dfw.printSchema()
root
 |-- id: string (nullable = true)
 |-- date: string (nullable = true)
 |-- dt: date (nullable = true)
 |-- ts: timestamp (nullable = true)
 |-- nested: struct (nullable = false)
 |    |-- dt: date (nullable = true)
 |    |-- ts: timestamp (nullable = true)
 |-- ctxSparkVersion: string (nullable = false)
 |-- argSparkVersion: string (nullable = false)
 |-- argHadoopVersion: string (nullable = false)
 |-- argTwitterParquetCommonVersion: string (nullable = false)
 |-- argTwitterParquetFormatVersion: string (nullable = false)
 |-- argTwitterParquetThriftVersion: string (nullable = false)
 |-- argApacheParquetCommonVersion: string (nullable = false)
 |-- argApacheParquetFormatVersion: string (nullable = false)
 |-- argApacheParquetThriftVersion: string (nullable = false)


scala> dfw.show(false)
+---+----------+----------+---------------------+----------------------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+
|id |date      |dt        |ts                   |nested                            |ctxSparkVersion|argSparkVersion|argHadoopVersion|argTwitterParquetCommonVersion|argTwitterParquetFormatVersion|argTwitterParquetThriftVersion|argApacheParquetCommonVersion|argApacheParquetFormatVersion|argApacheParquetThriftVersion|
+---+----------+----------+---------------------+----------------------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+
|1  |0000-01-01|0001-01-01|0001-01-01 00:00:00.0|[0001-01-01,0001-01-01 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|2  |0050-01-01|0050-01-01|0050-01-01 00:00:00.0|[0050-01-01,0050-01-01 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|3  |0999-12-31|0999-12-31|0999-12-31 00:00:00.0|[0999-12-31,0999-12-31 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|4  |1000-01-01|1000-01-01|1000-01-01 00:00:00.0|[1000-01-01,1000-01-01 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|5  |1592-10-14|1592-10-14|1592-10-14 00:00:00.0|[1592-10-14,1592-10-14 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|6  |1592-10-15|1592-10-15|1592-10-15 00:00:00.0|[1592-10-15,1592-10-15 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|7  |1899-12-31|1899-12-31|1899-12-31 00:00:00.0|[1899-12-31,1899-12-31 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|8  |1900-01-01|1900-01-01|1900-01-01 00:00:00.0|[1900-01-01,1900-01-01 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|9  |9999-12-31|9999-12-31|9999-12-31 00:00:00.0|[9999-12-31,9999-12-31 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
+---+----------+----------+---------------------+----------------------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+


scala> 

scala> val loc = s"/home/gmanche/data/testId=${testId}"
loc: String = /home/gmanche/data/testId=test__2.1.3__2.7.7__8__3

scala> 

scala> dfw.coalesce(1).write.format("parquet").mode("overwrite").save(loc)
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

scala> 

scala> val dfr = spark.read.format("parquet").load(loc)
dfr: org.apache.spark.sql.DataFrame = [id: string, date: string ... 12 more fields]

scala> 

scala> dfr.printSchema()
root
 |-- id: string (nullable = true)
 |-- date: string (nullable = true)
 |-- dt: date (nullable = true)
 |-- ts: timestamp (nullable = true)
 |-- nested: struct (nullable = true)
 |    |-- dt: date (nullable = true)
 |    |-- ts: timestamp (nullable = true)
 |-- ctxSparkVersion: string (nullable = true)
 |-- argSparkVersion: string (nullable = true)
 |-- argHadoopVersion: string (nullable = true)
 |-- argTwitterParquetCommonVersion: string (nullable = true)
 |-- argTwitterParquetFormatVersion: string (nullable = true)
 |-- argTwitterParquetThriftVersion: string (nullable = true)
 |-- argApacheParquetCommonVersion: string (nullable = true)
 |-- argApacheParquetFormatVersion: string (nullable = true)
 |-- argApacheParquetThriftVersion: string (nullable = true)


scala> dfr.show(false)
24/02/04 16:09:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
+---+----------+----------+---------------------+----------------------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+
|id |date      |dt        |ts                   |nested                            |ctxSparkVersion|argSparkVersion|argHadoopVersion|argTwitterParquetCommonVersion|argTwitterParquetFormatVersion|argTwitterParquetThriftVersion|argApacheParquetCommonVersion|argApacheParquetFormatVersion|argApacheParquetThriftVersion|
+---+----------+----------+---------------------+----------------------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+
|1  |0000-01-01|0001-01-01|0001-01-01 00:00:00.0|[0001-01-01,0001-01-01 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|2  |0050-01-01|0050-01-01|0050-01-01 00:00:00.0|[0050-01-01,0050-01-01 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|3  |0999-12-31|0999-12-31|0999-12-31 00:00:00.0|[0999-12-31,0999-12-31 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|4  |1000-01-01|1000-01-01|1000-01-01 00:00:00.0|[1000-01-01,1000-01-01 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|5  |1592-10-14|1592-10-14|1592-10-14 00:00:00.0|[1592-10-14,1592-10-14 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|6  |1592-10-15|1592-10-15|1592-10-15 00:00:00.0|[1592-10-15,1592-10-15 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|7  |1899-12-31|1899-12-31|1899-12-31 00:00:00.0|[1899-12-31,1899-12-31 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|8  |1900-01-01|1900-01-01|1900-01-01 00:00:00.0|[1900-01-01,1900-01-01 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
|9  |9999-12-31|9999-12-31|9999-12-31 00:00:00.0|[9999-12-31,9999-12-31 00:00:00.0]|2.1.3          |2.1.3          |2.7.7           |1.6.0                         |2.2.0rc1                      |0.7.0                         |1.8.1                        |2.3.0-incubating             |0.7.0                        |
+---+----------+----------+---------------------+----------------------------------+---------------+---------------+----------------+------------------------------+------------------------------+------------------------------+-----------------------------+-----------------------------+-----------------------------+


scala> :quit
